{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model evaluation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNZHRpuwRsQDCsNi1lqIg/V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mehrdad117/Brain-Age-prediction-/blob/main/model_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZxoW8EbEzkJ"
      },
      "source": [
        "!pip install tpot\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.datasets import load_boston\n",
        " \n",
        " \n",
        "import pandas as pd #this is how we usually import pandas\n",
        "import numpy as np #this is how we usually import numpy\n",
        " \n",
        "import matplotlib #only needed to determine Matplotlib version number\n",
        "#import tables # pytables is needed to read and write hdf5 files\n",
        "#import openpyxl # is used to read and write MS Excel files\n",
        "import scipy.stats as stats\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        " \n",
        "import xgboost\n",
        " \n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxXG3VK7E8Zm"
      },
      "source": [
        "**loading data**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0WBUcl7E2q8"
      },
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/Finalfinal.csv')\n",
        "\n",
        "data = data.drop('SubjectID' , axis=1)\n",
        "\n",
        "\n",
        "#if data contains sex feature\n",
        "#size_mapping = {\n",
        "     #   #   'M': 0,\n",
        "       #    'F': 1,\n",
        "        #         }\n",
        "#data['Sex'] = data['Sex'].map(size_mapping)\n",
        "\n",
        "\n",
        "features_all = data.drop('Age', axis=1)\n",
        "target = data['Age']\n",
        "target = target.values.reshape(-1,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6kksJW9E7il"
      },
      "source": [
        "features_all.shape , target.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ruxVg92FJif"
      },
      "source": [
        "# **split**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmjDdqLEFJ05"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(features_all, target,\n",
        "                                                    train_size=0.9, test_size=0.1, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ItTSVokFjhV"
      },
      "source": [
        "paste selected model here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7J5TBGEUFNMa"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.linear_model import ElasticNetCV, LassoLarsCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline, make_union\n",
        "from tpot.builtins import OneHotEncoder, StackingEstimator\n",
        "from tpot.export_utils import set_param_recursive\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from copy import copy\n",
        "\n",
        "model = make_pipeline(\n",
        "    make_union(\n",
        "        FunctionTransformer(copy),\n",
        "        StackingEstimator(estimator=make_pipeline(\n",
        "            StackingEstimator(estimator=AdaBoostRegressor(learning_rate=0.5, loss=\"square\", n_estimators=100)),\n",
        "            ElasticNetCV(l1_ratio=0.7000000000000001, tol=0.1)\n",
        "        ))\n",
        "    ),\n",
        "    StackingEstimator(estimator=ElasticNetCV(l1_ratio=0.15000000000000002, tol=0.1)),\n",
        "    StackingEstimator(estimator=LassoLarsCV(normalize=False)),\n",
        "    StackingEstimator(estimator=LassoLarsCV(normalize=False)),\n",
        "    OneHotEncoder(minimum_fraction=0.1, sparse=False, threshold=10),\n",
        "    VarianceThreshold(threshold=0.001),\n",
        "    StackingEstimator(estimator=GradientBoostingRegressor(alpha=0.75, learning_rate=0.5, loss=\"quantile\", max_depth=4, max_features=0.6500000000000001, min_samples_leaf=13, min_samples_split=2, n_estimators=100, subsample=0.05)),\n",
        "    LassoLarsCV(normalize=True)\n",
        ")\n",
        "# Fix random state for all the steps in exported pipeline\n",
        "set_param_recursive(exported_pipeline.steps, 'random_state', 42)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwsR8r9sGdxF"
      },
      "source": [
        "fitting model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaYCJcoSGdCS"
      },
      "source": [
        "model.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hF8x6j_8GlHd"
      },
      "source": [
        "**calculate MAE for train and test dataset to calculate overfitting / underfitting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rErO_2rbGkoH"
      },
      "source": [
        "def calc_train_error(X_train, y_train, model):\n",
        "    '''returns in-sample error for already fit model.'''\n",
        "    predictions = model.predict(X_train)\n",
        "    mae = mean_absolute_error(y_train, predictions)\n",
        "    return mae\n",
        "    \n",
        "def calc_validation_error(X_test, y_test, model):\n",
        "    '''returns out-of-sample error for already fit model.'''\n",
        "    predictions1 = model.predict(X_test)\n",
        "    mae = mean_absolute_error(y_test, predictions1)\n",
        "    return mae\n",
        "    \n",
        "def calc_metrics(X_train, y_train, X_test, y_test, model):\n",
        "    '''fits model and returns the RMSE for in-sample error and out-of-sample error'''\n",
        "    model.fit(X_train, y_train)\n",
        "    train_error = calc_train_error(X_train, y_train, model)\n",
        "    validation_error = calc_validation_error(X_test, y_test, model)\n",
        "    return train_error, validation_error\n",
        "\n",
        "\n",
        "train_error, test_error = calc_metrics(X_train, y_train, X_test, y_test, model)\n",
        "train_error, test_error = round(train_error, 3), round(test_error, 3)\n",
        "\n",
        "print('train error: {} | test error: {}'.format(train_error, test_error))\n",
        "print('train/test: {}'.format(round(test_error/train_error, 1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4FpUYW-HObD"
      },
      "source": [
        "# **Cross Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBU0ntFJHatT"
      },
      "source": [
        "import pandas as pd\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "\n",
        "# prepare the cross-validation procedure\n",
        "\n",
        "# two options for cv :\n",
        "\n",
        "cv1 = KFold(n_splits=10, random_state=42, shuffle=True)\n",
        "#LeaveOneOut()\n",
        "\n",
        "\n",
        "# evaluate model\n",
        "scores = cross_val_score(model, features_all, target, cv=LeaveOneOut() ,scoring='neg_mean_absolute_error', n_jobs=-1)\n",
        "\n",
        "\n",
        "cv_predicts_n_folds = cross_val_predict(model, features_all, target, cv=10)\n",
        "\n",
        "print(\"Accuracy score in each iteration: {}\".format(scores))\n",
        "print(\"Predicted class for each record: {}\".format(cv_predicts_n_folds))\n",
        "print(\"K-Fold Score: {}\".format(np.mean(scores)))\n",
        "\n",
        "# report performance\n",
        "print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OMAM6jhIqjA"
      },
      "source": [
        "# **learning curve**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcycfSiHI55Q"
      },
      "source": [
        "# #1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A01bT-49Iq3A"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import validation_curve\n",
        "from sklearn.model_selection import learning_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "train_sizes, train_scores, valid_scores = learning_curve(model, features_all, target, cv=10, scoring='neg_mean_absolute_error', n_jobs=-1, shuffle=True, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "train_scores_mean = -train_scores.mean(axis = 1)\n",
        "validation_scores_mean = -valid_scores.mean(axis = 1 )\n",
        "print('Mean training scores\\n\\n', pd.Series(train_scores_mean, index = train_sizes))\n",
        "print('\\n', '-' * 20) # separator\n",
        "print('\\nMean validation scores\\n\\n',pd.Series(validation_scores_mean, index = train_sizes))\n",
        "\n",
        "\n",
        "\n",
        "### Plotting the two learning curves ###\n",
        "\n",
        "plt.style.use('seaborn')\n",
        "plt.plot(train_sizes, train_scores_mean, label = 'Training error')\n",
        "plt.plot(train_sizes, validation_scores_mean, label = 'Validation error')\n",
        "plt.ylabel('MSE', fontsize = 14)\n",
        "plt.xlabel('Training set size', fontsize = 14)\n",
        "plt.title('Learning curves for a linear regression model', fontsize = 18, y = 1.03)\n",
        "plt.legend()\n",
        "plt.ylim(0,40)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1wEvGxII2Zx"
      },
      "source": [
        "# #2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQHXl0MLI11a"
      },
      "source": [
        "\n",
        "# Create means and standard deviations of training set scores\n",
        "train_mean = -np.mean(train_scores, axis=1)\n",
        "train_std = -np.std(train_scores, axis=1)\n",
        "\n",
        "# Create means and standard deviations of test set scores\n",
        "test_mean = -np.mean(valid_scores, axis=1)\n",
        "test_std = -np.std(valid_scores, axis=1)\n",
        "\n",
        "# Draw lines\n",
        "plt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\n",
        "plt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n",
        "\n",
        "# Draw bands\n",
        "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\n",
        "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n",
        "\n",
        "# Create plot\n",
        "plt.title(\"Learning Curve\")\n",
        "plt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}